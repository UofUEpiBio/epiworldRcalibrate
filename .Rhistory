usethis::use_testthat()
usethis::use_mit_license(copyright_holder = "George G. Vega Yon")
usethis::use_mit_license(copyright_holder = "Sima Njf")
library(epiworldR)
library(data.table)
library(tensorflow)
library(keras)
library(parallel)
library(keras3)
library(dplyr)
library(ggplot2)
# Source the data preparation file
source("calibration/dataprep.R")
# Source the data preparation file
source("R/calibration/dataprep.R")
# Function to generate theta parameters
generate_theta <- function(N, n) {
set.seed(1231)
theta <- data.table(
preval = sample((100:2000) / n, N, TRUE),
crate  = rgamma(N, 5, 1),
ptran  = rbeta(N, 3, 7),
prec   = rbeta(N, 10, 10*2 - 10)
)
return(theta)
}
# Function to run simulations
run_simulations <- function(N, n, ndays, ncores, theta, seeds) {
matrices <- parallel::mclapply(1:N, FUN = function(i) {
fn <- sprintf("calibration/simulated_data/sir-%06i.rds", i)
if (file.exists(fn))
return(readRDS(fn))
set.seed(seeds[i])
m <- theta[i, ModelSIRCONN(
"mycon",
prevalence        = preval,
contact_rate      = crate,
transmission_rate = ptran,
recovery_rate     = prec,
n                 = n
)]
verbose_off(m)
run(m, ndays = ndays)
ans <- prepare_data(m)
saveRDS(ans, fn)
return(ans)
}, mc.cores = ncores)
return(matrices)
}
# Function to filter non-null matrices and update theta
filter_non_null <- function(matrices, theta) {
is_not_null <- intersect(
which(!sapply(matrices, inherits, what = "error")),
which(!sapply(matrices, \(x) any(is.na(x))))
)
matrices <- matrices[is_not_null]
theta    <- theta[is_not_null, ]
return(list(matrices = matrices, theta = theta, N = length(is_not_null)))
}
# Prepare data for TensorFlow
prepare_data_for_tensorflow <- function(matrices, N) {
arrays_1d <- array(dim = c(N, dim(matrices[[1]][1,,])))
for (i in seq_along(matrices)) {
arrays_1d[i,,] <- matrices[[i]][1,,]
}
return(arrays_1d)
}
# Function to split data into training and test sets
split_data <- function(arrays_1d, theta2, N) {
N_train <- floor(N * 0.7)
id_train <- 1:N_train
id_test <- (N_train + 1):N
train <- list(
x = array_reshape(arrays_1d[id_train,,], dim = c(N_train, dim(arrays_1d)[-1])),
y = array_reshape(as.matrix(theta2)[id_train,], dim = c(N_train, ncol(theta2)))
)
test <- list(
x = array_reshape(arrays_1d[id_test,,], dim = c(N - N_train, dim(arrays_1d)[-1])),
y = array_reshape(as.matrix(theta2)[id_test,], dim = c(N - N_train, ncol(theta2)))
)
return(list(train = train, test = test))
}
# Building the CNN model
build_cnn_model <- function(input_shape, output_units) {
model <- keras3::keras_model_sequential() %>%
keras3::layer_conv_2d(
filters = 32,
input_shape = c(input_shape, 1),
activation = "linear",
kernel_size = c(3, 5)
) %>%
keras3::layer_max_pooling_2d(pool_size = 2, padding = 'same') %>%
keras3::layer_flatten(input_shape = input_shape) %>%
keras3::layer_dense(units = output_units, activation = 'sigmoid')
model %>% compile(optimizer = 'adam', loss = 'mse', metric = 'accuracy')
return(model)
}
# Function to train the CNN model
train_model <- function(model, train_data, epochs = 100) {
tensorflow::set_random_seed(331)
model %>% fit(train_data$x, train_data$y, epochs = epochs, verbose = 0)
}
# Function to evaluate the CNN model
evaluate_model <- function(model, test_data, theta) {
pred <- predict(model, x = test_data$x) |>
as.data.table() |>
setnames(colnames(theta))
MAEs <- abs(pred - as.matrix(test_data$y)) |> colMeans() |> print()
return(list(pred = pred, MAEs = MAEs))
}
# Function to plot the results
plot_results <- function(pred, test_data, theta, MAEs, N, N_train) {
pred[, id := 1L:.N]
pred[, crate := qlogis(crate) * 10]
pred_long <- melt(pred, id.vars = "id")
theta_long <- test_data$y |> as.data.table()
setnames(theta_long, names(theta))
theta_long[, id := 1L:.N]
theta_long[, crate := qlogis(crate) * 10]
theta_long <- melt(theta_long, id.vars = "id")
alldat <- rbind(
cbind(pred_long, Type = "Predicted"),
cbind(theta_long, Type = "Observed")
)
p1 <- ggplot(alldat, aes(x = value, colour = Type)) +
facet_wrap(~variable, scales = "free") +
geom_boxplot() +
labs(title = "Boxplot: Predicted vs Observed")
print(p1)
alldat_wide <- dcast(alldat, id + variable ~ Type, value.var = "value")
vnames <- data.table(
variable = c("preval", "crate", "ptran", "prec"),
Name     = paste(
c("Init. state", "Contact Rate", "P(transmit)", "P(recover)"),
sprintf("(MAE: %.2f)", MAEs)
)
)
alldat_wide <- merge(alldat_wide, vnames, by = "variable")
p2 <- ggplot(alldat_wide, aes(x = Observed, y = Predicted)) +
facet_wrap(~ Name, scales = "free") +
geom_abline(slope = 1, intercept = 0) +
geom_point(alpha = .2) +
labs(
title    = "Observed vs Predicted (validation set)",
subtitle = sprintf(
"The model includes %i simulated datasets, of which %i were used for training.",
N, N_train
),
caption  = "Predictions made using a CNN as implemented with loss function MAE."
)
print(p2)
}
# Main function to run the entire pipeline
main_pipeline2 <- function(N,n,ndays,ncores) {
theta <- generate_theta(N, n)
seeds <- sample.int(.Machine$integer.max, N, TRUE)
matrices <- run_simulations(N, n, ndays, ncores, theta, seeds)
filtered_data <- filter_non_null(matrices, theta)
matrices <- filtered_data$matrices
theta <- filtered_data$theta
N <- filtered_data$N
arrays_1d <- prepare_data_for_tensorflow(matrices, N)
theta2 <- copy(theta)
theta2[, crate := plogis(crate / 10)]
saveRDS(list(theta = theta2, simulations = arrays_1d), file = "calibration/sir.rds", compress = TRUE)
data_split <- split_data(arrays_1d, theta2, N)
train <- data_split$train
test <- data_split$test
model <- build_cnn_model(dim(arrays_1d)[-1], ncol(theta2))
train_model(model, train)
eval_results <- evaluate_model(model, test, theta)
pred <- eval_results$pred
MAEs <- eval_results$MAEs
plot_results(pred, test, theta, MAEs, N, floor(N * 0.7))
}
library(tensorflow)
# Run the main pipeline
N <- 2e4
n <- 5000
ndays <- 50
ncores <- 20
devtools::document()
library(myfirst.package)
library(myfirst.package)
